{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIqiFYE0l9SP"
      },
      "source": [
        "\n",
        "**Neural Network Classifier for eICU Patient Data**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajaJTKPBl5YA"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNFKLOlwBCID",
        "outputId": "2a58c544-8079-4719-b746-bd83feee8b34"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwH_LgPrdMNH",
        "outputId": "146765f1-472d-4b23-9304-ba8784f1c34f"
      },
      "outputs": [],
      "source": [
        "pip install -r \"/content/gdrive/MyDrive/MA_XAI/CTdata/Brain_Hemorage_python/requirements.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWdr1IUmJOLb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import gc\n",
        "\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import xgboost\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data import sampler\n",
        "from torchvision import transforms\n",
        "import fastprogress\n",
        "import time\n",
        "import random\n",
        "from datetime import date\n",
        "\n",
        "\n",
        "\n",
        "# imports from captum library\n",
        "#from captum.attr import (\n",
        "#    IntegratedGradients,\n",
        "#    DeepLift,\n",
        "#    KernelShap,\n",
        "#    LRP,\n",
        "#    Lime,\n",
        "#    visualization)\n",
        "from captum._utils.models.linear_model import SkLearnLinearRegression, SkLearnLasso\n",
        "\n",
        "\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "import quantus\n",
        "import shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNipKPxrrd1e"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGv8llYXrgKX"
      },
      "outputs": [],
      "source": [
        "import utils_MA\n",
        "import ExplanationWrapper_tabular\n",
        "from utils_MA import get_device, CustomPatientData, MLP, accuracy, train2, validate2, run_training2, plot, test, EarlyStopper\n",
        "from ExplanationWrapper_tabular import (explainer_wrapper,\n",
        "                                       lime_explainer,\n",
        "                                       kernelshap_explainer,\n",
        "                                       lrp_explainer,\n",
        "                                       deeplift_explainer,\n",
        "                                       intgrad_explainer,\n",
        "                                       random_explainer,\n",
        "                                       custom_perturbation_func,\n",
        "                                       radar_factory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2xh5rdGDVJf"
      },
      "source": [
        "System checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7YfYY5cEN6P",
        "outputId": "5bd47e42-4a52-4699-c93d-a8fa86b90ef2"
      },
      "outputs": [],
      "source": [
        "device = get_device()\n",
        "\n",
        "# Get number of cpus to use for faster parallelized data loading\n",
        "num_cpus = os.cpu_count()\n",
        "print(num_cpus, 'CPUs available')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulY5Kk1Q9fEr"
      },
      "source": [
        "Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XM-Bp4f-NF4",
        "outputId": "d0610d18-dd98-4273-9a7f-c689a91ec726"
      },
      "outputs": [],
      "source": [
        "orig_data = pd.read_csv(\"/content/gdrive/MyDrive/MA_XAI/PatientData_ready/final_eICU_data.csv\")\n",
        "\n",
        "# specify target variable Y and remove variables not used for training\n",
        "Y_df = orig_data['intubated']\n",
        "useddata = orig_data.drop(['intubated','patientunitstayid',\n",
        "                           'oxygen_therapy_type', #'supp_oxygen',\n",
        "                           'icu_los','unitdischargestatus',\n",
        "                           'vent_start'],axis = 1)\n",
        "\n",
        "# Label NA values in 'vassopressor' such that they are modeled as own category\n",
        "useddata['vasopressor'] = useddata['vasopressor'].fillna(0)\n",
        "\n",
        "# specify categorical variables as 'category'\n",
        "cat_vals = ['specialty']\n",
        "\n",
        "\n",
        "# check shares of missing values within each feature\n",
        "print(useddata.isnull().mean().sort_values(ascending=False))\n",
        "\n",
        "\n",
        "\"\"\" Imputation - KNN imputation for missing values for\n",
        "    SpO2, gcs, glucose, PaO2, PaCO2, or HCO3\n",
        "    Using the KNNImputer from sklearn \"\"\"\n",
        "\n",
        "imp = KNNImputer()\n",
        "\n",
        "# impute missing data\n",
        "imputed_X = imp.fit_transform(useddata)\n",
        "X_df = pd.DataFrame(imputed_X, columns = useddata.columns)\n",
        "\n",
        "X_df['gcs'] = round(X_df['gcs'],0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpwO4tsqpeas"
      },
      "outputs": [],
      "source": [
        "# Group specialty variable and assign meaningful lables\n",
        "\n",
        "# Specialty\n",
        "specialty_conditions = [\n",
        "    (X_df['specialty'] == 0),\n",
        "    (X_df['specialty'] == 1),\n",
        "    (X_df['specialty'] == 2)\n",
        "]\n",
        "specialty_values = ['Spec:Surgery','Spec:Other','Spec:Medical']\n",
        "X_df['specialty_grouped'] = np.select(specialty_conditions, specialty_values)\n",
        "\n",
        "\n",
        "# Data preparation\n",
        "specialty = pd.get_dummies(X_df['specialty_grouped'],drop_first = True)\n",
        "X_df.drop(['specialty','specialty_grouped'],axis = 1, inplace=True)\n",
        "X_df = pd.concat([X_df,specialty],axis = 1)\n",
        "colnames = X_df.columns\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLpCg8vm-aF0"
      },
      "source": [
        "Normalize data and transform to tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cfzup2ea-ZfA"
      },
      "outputs": [],
      "source": [
        "# TODO calculate mean and standard deviation of train set\n",
        "meanX = X_df.mean(axis = 0)\n",
        "stdX = X_df.std(axis = 0)\n",
        "\n",
        "X_norm = (X_df - meanX) / stdX\n",
        "\n",
        "# convert to torch tensors\n",
        "\n",
        "X = torch.tensor(X_norm.values.astype(np.float32))\n",
        "Y = torch.tensor(Y_df.astype(np.int64).values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX5JyeBqRCpS"
      },
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "hL2HtD3gQzJR",
        "outputId": "7632c918-12a9-4d33-d75f-df7e3c4a5c1b"
      },
      "outputs": [],
      "source": [
        "# Estimation\n",
        "model = LogisticRegression(fit_intercept = True, C = 1,max_iter = 10000)\n",
        "mdl = model.fit(X_norm, Y_df)\n",
        "pred = mdl.predict(X_norm)\n",
        "cm = metrics.confusion_matrix(Y_df, pred)\n",
        "\n",
        "display(cm)\n",
        "# norm coefficents and store them in data frame\n",
        "coef = model.coef_[0]\n",
        "coef_X_input = (coef*X_norm) # mulitply coeficients of logistic regression with inputs\n",
        "normed_coef_X_input = coef_X_input / np.linalg.norm(coef_X_input, ord=1)\n",
        "\n",
        "#normed_coef = coef / np.linalg.norm(coef, ord=1)\n",
        "logit_coef = pd.DataFrame({'Features':X_norm.columns,'variable': 'Logistic Regression', 'value': coef})\n",
        "abs_logit_coef = pd.DataFrame({'Features':X_norm.columns,'variable': 'Logistic Regression', 'value': abs(coef)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3ChzVkSyl2l"
      },
      "source": [
        "**Weight initialization for MLP model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgARjgiBygJi"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_normal_(m.weight)\n",
        "        m.bias.data.fill_(0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWPI7m01W-43"
      },
      "source": [
        "Run training 'nr_iter' times and save loss, AUC, accuracy, XAI attributions, predictions and labels for every iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCUN5k41YffA"
      },
      "outputs": [],
      "source": [
        "\"\"\" Set hyperparamters for training \"\"\"\n",
        "nr_iter = 100\n",
        "batch_size = 16\n",
        "\n",
        "# network architecture\n",
        "num_hidden_units = 50\n",
        "num_hidden_layers = 1\n",
        "dropout = .05\n",
        "\n",
        "# early stopper\n",
        "patience_param = 5\n",
        "\n",
        "# training\n",
        "loss_function = torch.nn.MSELoss()\n",
        "num_epochs = 30\n",
        "lr = 0.0001\n",
        "\n",
        "# evaluation\n",
        "threshold = 0.3\n",
        "\n",
        "# xai methods\n",
        "xai_methods = ['Lime','KernelShap','LRP','DeepLIFT','IntegratedGradients']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYHAUPwxm3qh"
      },
      "outputs": [],
      "source": [
        "def define_eval_metrics():\n",
        "\n",
        "  eval_metrics = {\n",
        "      \"LocalLipSchitz\": quantus.LocalLipschitzEstimate(\n",
        "          nr_samples=20,\n",
        "          norm_numerator=quantus.similarity_func.distance_euclidean,\n",
        "          norm_denominator=quantus.similarity_func.distance_euclidean,    \n",
        "          perturb_func=custom_perturbation_func,\n",
        "          perturb_func_kwargs = {'sets':sets, 'len_sets':len_sets},\n",
        "          similarity_func=quantus.similarity_func.lipschitz_constant,\n",
        "          aggregate_func=np.mean,\n",
        "          return_aggregate=True,\n",
        "          return_nan_when_prediction_changes = True,\n",
        "          disable_warnings = True\n",
        "          \n",
        "      ),\n",
        "      \"AvgSensitivity\": quantus.AvgSensitivity(\n",
        "          nr_samples=20,\n",
        "          lower_bound=0.2,\n",
        "          norm_numerator=quantus.norm_func.fro_norm,\n",
        "          norm_denominator=quantus.norm_func.fro_norm,\n",
        "          perturb_func=custom_perturbation_func,\n",
        "          perturb_func_kwargs = {\"sets\":sets, \"len_sets\": len_sets},\n",
        "          similarity_func=quantus.similarity_func.difference,\n",
        "          abs=False,\n",
        "          normalise=False,\n",
        "          aggregate_func=np.mean,\n",
        "          return_aggregate=True,\n",
        "          disable_warnings = True\n",
        "      ),\n",
        "      \"Faithfulness_Correlation\": quantus.FaithfulnessCorrelation(\n",
        "          nr_runs=30,\n",
        "          subset_size = 5,\n",
        "          perturb_func=quantus.perturb_func.baseline_replacement_by_indices,\n",
        "          perturb_func_kwargs = {'sets':sets, 'len_sets':len_sets, 'use_baseline':True, 'baseline':baseline_median},\n",
        "          similarity_func=quantus.similarity_func.correlation_pearson,\n",
        "          abs=False,\n",
        "          normalise=False,\n",
        "          aggregate_func=np.mean,\n",
        "          return_aggregate=True,\n",
        "          disable_warnings = True\n",
        "      ),\n",
        "      \"Faithfulness_Estimate\": quantus.FaithfulnessEstimate(\n",
        "          perturb_func=quantus.perturb_func.baseline_replacement_by_indices,\n",
        "          perturb_func_kwargs = {'sets':sets, 'len_sets':len_sets, 'use_baseline':True, 'baseline':baseline_median},\n",
        "          similarity_func=quantus.similarity_func.correlation_pearson,\n",
        "          aggregate_func=np.mean,\n",
        "          features_in_step=2,  \n",
        "          return_aggregate=True,\n",
        "          abs = False,\n",
        "          disable_warnings = True\n",
        "      ),\n",
        "      \"Complexity\": quantus.Sparseness(\n",
        "          abs=True,\n",
        "          normalise=False,\n",
        "          aggregate_func=np.mean,\n",
        "          return_aggregate=True,\n",
        "          disable_warnings = True\n",
        "      ),\n",
        "      \"Randomisation\": quantus.ModelParameterRandomisation(\n",
        "          similarity_func=quantus.similarity_func.correlation_spearman,\n",
        "          return_sample_correlation=True,\n",
        "          return_aggregate=True,\n",
        "          aggregate_func=np.mean,\n",
        "          layer_order=\"independent\",\n",
        "          disable_warnings=True,\n",
        "          normalise=True,\n",
        "          abs=True,\n",
        "      ),\n",
        "  }\n",
        "\n",
        "  return eval_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "QzPG8yeeW464",
        "outputId": "6daa6c29-9733-4221-c18f-a6f96e01011e"
      },
      "outputs": [],
      "source": [
        "# lists to save training metrics for each iteration\n",
        "TRAIN_LOSSES, TRAIN_ACCS, VAL_LOSSES, VAL_ACCS, VAL_AUCS, EPOCHS_STOPPED = [], [], [], [], [], []\n",
        "CONFUSION_MATRIX, YS, Y_PREDS = [], [], []\n",
        "\n",
        "# lists to save attributions each iteration\n",
        "RANDOM_ATTR, LIME_ATTR, KSHAP_ATTR, LRP_ATTR, DeepLIFT_ATTR, IG_ATTR  = [],[],[],[],[],[]\n",
        "IG_ATTR_TP, IG_ATTR_FP, IG_ATTR_TN, IG_ATTR_FN = [],[],[],[]\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" median-mode baseline \"\"\"\n",
        "baseline_median = X.median(dim = 0)[0].unsqueeze(dim = 0).reshape(1,1,20)\n",
        "\n",
        "for i in range(nr_iter):\n",
        "  \n",
        "  # Different random seed each iteration\n",
        "  RANDOM_SEED = i\n",
        "  RANDOM = np.random.RandomState(RANDOM_SEED)\n",
        "\n",
        "  \"\"\" \n",
        "  Apply Monte Carlo Cross Validation:\n",
        "  In each iteration a new training and validation set is created\n",
        "  \"\"\"\n",
        "  X_train, X_valid, Y_train, Y_valid = train_test_split(X,Y, test_size = 0.1,shuffle = True, random_state = RANDOM)\n",
        "  traindata, valdata = CustomPatientData(X_train, Y_train), CustomPatientData(X_valid, Y_valid)\n",
        "  traindl, valdl = DataLoader(traindata, batch_size = batch_size, shuffle=False, drop_last = False), DataLoader(valdata, batch_size = batch_size, shuffle=False, drop_last = False)\n",
        "\n",
        "  \"\"\" instantiate model \"\"\"\n",
        "  model = MLP(num_features = 20, dropout = dropout,  \n",
        "            num_hidden_units = num_hidden_units, num_hidden_layers = num_hidden_layers)\n",
        "\n",
        "\n",
        "  \"\"\" specify model and optimizer \"\"\"\n",
        "  model = model.to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  \n",
        "\n",
        "  \"\"\" instantiate early stopper \"\"\"\n",
        "  stopper = EarlyStopper(patience=patience_param)\n",
        "\n",
        "\n",
        "  \"\"\" run training \"\"\"\n",
        "  train_losses, val_losses, train_accs, val_accs, val_AUC, epoch, confusion_matrix, y, y_pred = run_training2(model, optimizer, loss_function, device, num_epochs, \n",
        "                traindl, valdl, lr, early_stopper=stopper, verbose=False, threshold = threshold)\n",
        "\n",
        "\n",
        "  \"\"\" Only keep metrics from best model \"\"\"\n",
        "  z = epoch - patience_param\n",
        "  TRAIN_ACCS.append(train_accs[z])\n",
        "  TRAIN_LOSSES.append(train_losses[z])\n",
        "  EPOCHS_STOPPED.append(epoch)\n",
        "  VAL_LOSSES.append(val_losses[z])\n",
        "  VAL_ACCS.append(val_accs[z])\n",
        "  VAL_AUCS.append(val_AUC[z])\n",
        "  CONFUSION_MATRIX.append(confusion_matrix[z])\n",
        "\n",
        "  \"\"\" Label TruePositives, FalsePositives etc. \"\"\"\n",
        "  tp = np.where( (y[z] == 1) & (y_pred[z] == 1), True, False)\n",
        "  fp = np.where( (y[z] == 0) & (y_pred[z] == 1), True, False)\n",
        "  tn = np.where( (y[z] == 0) & (y_pred[z] == 0), True, False)\n",
        "  fn = np.where( (y[z] == 1) & (y_pred[z] == 0), True, False)\n",
        "  \n",
        "\n",
        "  \"\"\" compute XAI attributions \"\"\"\n",
        "  x_batch = X_valid.reshape(len(X_valid),1,20) #bring X_valid in right format for Quantus\n",
        "  sets = [np.unique(x_batch[:,:,i].cpu().detach().numpy()) for i  in range(20)] # for perturbation function\n",
        "  len_sets = np.asarray([len(sets[i]) for i in range(20)])\n",
        "\n",
        "  \n",
        "  \n",
        "  \"\"\" Random explanation as control \"\"\"\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  RANDOM_ATTR.append(random_explainer(model=model,\n",
        "                                inputs=x_batch,\n",
        "                                **{\"device\": device},))\n",
        "  \n",
        "  \n",
        "  \n",
        "  \"\"\" LIME \"\"\"\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  st = time.time()\n",
        "  LIME_ATTR.append(lime_explainer(model=model,\n",
        "                                  inputs=x_batch,\n",
        "                                  baseline = baseline_median,\n",
        "                                  **{\"device\": device},))\n",
        "  et = time.time()\n",
        "  lime_time = et - st\n",
        "\n",
        "  \n",
        "  \"\"\" Kernel Shap \"\"\"\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  st = time.time()\n",
        "  KSHAP_ATTR.append(kernelshap_explainer(model=model,\n",
        "                                         inputs=x_batch,\n",
        "                                         baseline = baseline_median,\n",
        "                                         **{\"device\": device},))\n",
        "  et = time.time()\n",
        "  kshap_time = et - st\n",
        "\n",
        "  \n",
        "  x_batch.requires_grad_()  # for the gradient based methods\n",
        "  \n",
        "  \"\"\" LRP \"\"\"\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  st = time.time()\n",
        "  LRP_ATTR.append(lrp_explainer(model=model,\n",
        "                                inputs=x_batch,\n",
        "                                **{\"device\": device},))\n",
        "  et = time.time()\n",
        "  lrp_time = et - st\n",
        "\n",
        "  \"\"\" DeepLIFT \"\"\"\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  st = time.time()\n",
        "  DeepLIFT_ATTR.append(deeplift_explainer(model=model,\n",
        "                                          inputs=x_batch,\n",
        "                                          baseline = baseline_median,\n",
        "                                          **{\"device\": device},))\n",
        "  et = time.time()\n",
        "  deeplift_time = et - st\n",
        "  \n",
        "  \n",
        "  \"\"\" Integrated Gradient \"\"\"\n",
        "  # for addditonal analysis distinguish attributions for IG between true positives, false positives etc, ....\n",
        "\n",
        "  # complete data\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  st = time.time()\n",
        "  IG_ATTR.append(intgrad_explainer(model=model,\n",
        "                                    inputs=x_batch,\n",
        "                                    baseline = baseline_median,\n",
        "                                    **{\"device\": device, \"n_steps\": 30},))\n",
        "  et = time.time()\n",
        "  ig_time = et - st\n",
        "  \n",
        "  \n",
        "  # true positives\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  IG_ATTR_TP.append(intgrad_explainer(model=model,\n",
        "                                     inputs=x_batch[tp],\n",
        "                                     baseline = baseline_median,\n",
        "                                     **{\"device\": device, \"n_steps\": 30},))\n",
        "  \n",
        "  # false positives\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  IG_ATTR_FP.append(intgrad_explainer(model=model,\n",
        "                                     inputs=x_batch[fp],\n",
        "                                     baseline = baseline_median,\n",
        "                                     **{\"device\": device, \"n_steps\": 30},))\n",
        "  \n",
        "  # true negatives\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  IG_ATTR_TN.append(intgrad_explainer(model=model,\n",
        "                                     inputs=x_batch[tn],\n",
        "                                     baseline = baseline_median,\n",
        "                                     **{\"device\": device, \"n_steps\": 30},))\n",
        "  \n",
        "  # false negatives\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  IG_ATTR_FN.append(intgrad_explainer(model=model,\n",
        "                                     inputs=x_batch[fn],\n",
        "                                     baseline = baseline_median,\n",
        "                                     **{\"device\": device, \"n_steps\": 30},))\n",
        "  \n",
        "\n",
        "  durations = {\n",
        "    \"Lime\": lime_time,\n",
        "    \"KernelShap\": kshap_time,\n",
        "    \"LRP\": lrp_time,\n",
        "    \"DeepLIFT\": deeplift_time,\n",
        "    \"IntegratedGradients\": ig_time\n",
        "    }\n",
        "\n",
        "  \"\"\" Evaluate attributions \"\"\"\n",
        "  eval_metrics = define_eval_metrics()\n",
        "\n",
        "  results = {method : {} for method in xai_methods}\n",
        "\n",
        "  for method in xai_methods:\n",
        "      for metric, metric_func in eval_metrics.items():\n",
        "          print(f\"Evaluating {metric} of {method} method.\")\n",
        "          gc.collect()\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "          # Get scores and append results.\n",
        "          scores = metric_func(\n",
        "              model=model,\n",
        "              x_batch=x_batch.cpu().detach().numpy(),\n",
        "              y_batch=0,\n",
        "              a_batch=None,\n",
        "              device=device,\n",
        "              explain_func=explainer_wrapper,\n",
        "              explain_func_kwargs={\n",
        "                  \"method\": method,\n",
        "                  \"device\": device,\n",
        "                  \"baseline\":baseline_median,\n",
        "                  \"n_steps\": 30,         \n",
        "              },\n",
        "          )\n",
        "          results[method][metric] = scores\n",
        "\n",
        "          # Empty cache.\n",
        "          gc.collect()\n",
        "          torch.cuda.empty_cache()\n",
        "        \n",
        "      results[method]['ComputationTime'] = durations.get(method) \n",
        " \n",
        "\n",
        "  # Postprocessing of scores: to get how the different explanation methods rank across criteria.\n",
        "  results_agg = {}\n",
        "  for method in xai_methods:\n",
        "      results_agg[method] = {}\n",
        "      for metric, metric_func in eval_metrics.items():\n",
        "          results_agg[method][metric] = np.mean(results[method][metric])\n",
        "      results_agg[method]['ComputationTime'] = results[method]['ComputationTime']\n",
        "\n",
        "  if i == 0:\n",
        "    df0 = pd.DataFrame.from_dict(results_agg)\n",
        "    df0 = df0.T.abs()\n",
        "    df0['Run'] = i+1\n",
        "    df_all = df0.copy()\n",
        "  else:\n",
        "    dfi = pd.DataFrame.from_dict(results_agg)\n",
        "    dfi = dfi.T.abs()\n",
        "    dfi['Run'] = i+1\n",
        "    df_all = df_all.append(dfi)\n",
        "\n",
        "\n",
        "\n",
        "LIME_ATTR, KSHAP_ATTR = np.stack(LIME_ATTR), np.stack(KSHAP_ATTR)\n",
        "LRP_ATTR, DeepLIFT_ATTR, IG_ATTR  =  np.stack(LRP_ATTR), np.stack(DeepLIFT_ATTR), np.stack(IG_ATTR)\n",
        "IG_ATTR_TP, IG_ATTR_FP, IG_ATTR_TN, IG_ATTR_FN = np.concatenate(IG_ATTR_TP), np.concatenate(IG_ATTR_FP), np.concatenate(IG_ATTR_TN), np.concatenate(IG_ATTR_FN)\n",
        "\n",
        "\n",
        "# Save explanations to file.\n",
        "explanations = {\n",
        "    \"Lime\": LIME_ATTR,\n",
        "    \"KernelShap\": KSHAP_ATTR,\n",
        "    \"LRP\": LRP_ATTR,\n",
        "    \"DeepLIFT\": DeepLIFT_ATTR,\n",
        "    \"IntegratedGradients\": IG_ATTR\n",
        "}\n",
        "\n",
        "explanations_by_group = {\n",
        "    \"True Positive\": IG_ATTR_TP,\n",
        "    \"False Positive\": IG_ATTR_FP,\n",
        "    \"True Negative\": IG_ATTR_TN,\n",
        "    \"False Negative\": IG_ATTR_FN\n",
        "}\n",
        "\n",
        "# Save train & test metrics in data frame\n",
        "metrics_list = [TRAIN_LOSSES, TRAIN_ACCS, VAL_LOSSES, VAL_ACCS, VAL_AUCS, EPOCHS_STOPPED]\n",
        "metrics_df = pd.DataFrame({'Train_Loss':metrics_list[0],'Train_ACC':metrics_list[1], 'Val_Loss':metrics_list[2], 'Val_ACC':metrics_list[3], 'Val_AUC':metrics_list[4],\n",
        "                            'Epoch':metrics_list[5]})\n",
        "\n",
        "\n",
        "\"\"\" save metrics, evaluation results and explanations to disk \"\"\"\n",
        "\n",
        "day = date.today().strftime(\"%m%d\")\n",
        "\n",
        "explanations_save_name = 'explanations_%s.pkl' % day\n",
        "pickle_file = F\"/content/gdrive/MyDrive/MA_XAI/PatientData_ready/Results/{explanations_save_name}\"\n",
        "with open(pickle_file, 'wb') as f0:\n",
        "    pickle.dump(explanations, f0)\n",
        "\n",
        "explanations_save_name2 = 'explanations_by_group_%s.pkl' % day\n",
        "pickle_file = F\"/content/gdrive/MyDrive/MA_XAI/PatientData_ready/Results/{explanations_save_name2}\"\n",
        "with open(pickle_file, 'wb') as f1:\n",
        "    pickle.dump(explanations_by_group, f1)\n",
        "\n",
        "df_all_save_name = 'df_all_%s.csv' % day\n",
        "df_all.to_csv(\"/content/gdrive/MyDrive/MA_XAI/PatientData_ready/Results/%s\" % df_all_save_name)\n",
        "\n",
        "metrics_df_save_name = 'metrics_results_%s.csv' % day\n",
        "metrics_df.to_csv(\"/content/gdrive/MyDrive/MA_XAI/PatientData_ready/Results/%s\" % metrics_df_save_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGjAsDxs_Gcz"
      },
      "outputs": [],
      "source": [
        "# Load saved results\n",
        "\n",
        "day = '0207'\n",
        "\n",
        "df_metrics = pd.read_csv(F\"/content/gdrive/MyDrive/MA_XAI/PatientData_ready/Results/metrics_results_{day}.csv\",index_col = 0)\n",
        "df_scores = pd.read_csv(F\"/content/gdrive/MyDrive/MA_XAI/PatientData_ready/Results/df_all_{day}.csv\", index_col = 0)\n",
        "\n",
        "avgs = df_scores.groupby(level=0,sort=False)\n",
        "df_avg = avgs.agg({'Faithfulness_Correlation':'mean', 'Faithfulness_Estimate':'mean','LocalLipSchitz':'mean', 'AvgSensitivity':'mean', 'Complexity':'mean', 'Randomisation':'mean', 'ComputationTime':'mean'})\n",
        "df_avg\n",
        "\n",
        "# Take inverse ranking for Robustness, since lower is better.\n",
        "df_normalised = df_avg.loc[:, ((df_avg.columns != 'LocalLipSchitz') & (df_avg.columns != 'AvgSensitivity') & (df_avg.columns != 'ComputationTime')) ].apply(lambda x: x / x.max())\n",
        "df_normalised[\"LocalLipSchitz\"] = df_avg[\"LocalLipSchitz\"].min()/df_avg[\"LocalLipSchitz\"].values\n",
        "df_normalised[\"AvgSensitivity\"] = df_avg[\"AvgSensitivity\"].min()/df_avg[\"AvgSensitivity\"].values\n",
        "df_normalised[\"ComputationTime\"] = df_avg[\"ComputationTime\"].min()/df_avg[\"ComputationTime\"].values\n",
        "df_normalised_rank = df_normalised.rank()\n",
        "df_normalised_rank\n",
        "\n",
        "# Load attributions\n",
        "explanations_save_name = 'explanations_%s.pkl' % day\n",
        "pickle_file = F\"/content/gdrive/MyDrive/MA_XAI/PatientData_ready/Results/{explanations_save_name}\"\n",
        "with open(pickle_file, 'rb') as a0:\n",
        "    attributions = pickle.load(a0)\n",
        "\n",
        "explanations_save_name2 = 'explanations_by_group_%s.pkl' % day\n",
        "pickle_file = F\"/content/gdrive/MyDrive/MA_XAI/PatientData_ready/Results/{explanations_save_name2}\"\n",
        "with open(pickle_file, 'rb') as a1:\n",
        "    attributions_by_group = pickle.load(a1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2vutbCV3aFC"
      },
      "outputs": [],
      "source": [
        "sns.set()\n",
        "\n",
        "df_normalised_rank = df_normalised_rank.iloc[:5,:]\n",
        "df_for_plots = df_normalised_rank[['Faithfulness_Correlation', 'Complexity', 'Randomisation', 'LocalLipSchitz', 'ComputationTime']]\n",
        "\n",
        "# Plotting configs.\n",
        "sns.set(font_scale=1)\n",
        "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
        "plt.rcParams['ytick.labelleft'] = True\n",
        "plt.rcParams['xtick.labelbottom'] = True\n",
        "\n",
        "include_titles = True\n",
        "include_legend = True\n",
        "\n",
        "\n",
        "# Plotting configs.\n",
        "colours_order = [\"#008080\", \"#FFA500\", \"#124E78\", \"#d62728\",'purple']\n",
        "methods_order = ['Faithfulness_Correlation', 'Complexity', 'Randomisation', 'LocalLipSchitz', 'ComputationTime']\n",
        "\n",
        "plt.rcParams['ytick.left'] = True\n",
        "plt.rcParams['ytick.labelleft'] = True\n",
        "plt.rcParams['xtick.bottom'] = True\n",
        "plt.rcParams['xtick.labelbottom'] = True\n",
        "include_titles = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "AXVLQzUZB83x",
        "outputId": "6aea08d8-8486-4c3f-bba4-a8b16661a8dd"
      },
      "outputs": [],
      "source": [
        "# Make spyder graph!\n",
        "data = [df_for_plots.columns.values, (df_for_plots.to_numpy())]\n",
        "theta = radar_factory(len(data[0]), frame='polygon')\n",
        "spoke_labels = data.pop(0)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='radar'))\n",
        "fig.subplots_adjust(top=0.85, bottom=0.05)\n",
        "for i, (d, method) in enumerate(zip(data[0], xai_methods)):\n",
        "    line = ax.plot(theta, d, label=method, color=colours_order[i], linewidth=5.0)\n",
        "    ax.fill(theta, d, alpha=0.15)\n",
        "\n",
        "# Set lables.\n",
        "if include_titles:\n",
        "  ax.set_varlabels(labels=['\\nFaithfulness', 'Complexity\\n', '\\nRandomisation', '\\nStability', 'Speed\\n'])\n",
        "else:\n",
        "  ax.set_varlabels(labels=[]) \n",
        "\n",
        "ax.set_rgrids(np.arange(0, df_for_plots.values.max() + 0.5), labels=[]) \n",
        "\n",
        "# Set a title.\n",
        "ax.set_title(\"Summary of Explanation Quantification\",  position=(0.5, 1.1), ha='center', fontsize=15)\n",
        "\n",
        "# Put a legend to the right of the current axis.\n",
        "if include_legend:\n",
        "    ax.legend(loc='upper left', bbox_to_anchor=(1, 0.5),fontsize = 15)\n",
        "\n",
        "plt.tight_layout()\n",
        "#plt.savefig('/content/gdrive/MyDrive/MA_XAI/PatientData_ready/Results/Spiderplot.png',bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puMY9UK-qmB4"
      },
      "outputs": [],
      "source": [
        "LIME_ATTR = attributions.get('Lime').reshape(attributions.get('Lime').shape[0]* attributions.get('Lime').shape[1],20)\n",
        "KS_ATTR = attributions.get('KernelShap').reshape(attributions.get('KernelShap').shape[0]* attributions.get('KernelShap').shape[1],20)\n",
        "LRP_ATTR = attributions.get('LRP').reshape(attributions.get('LRP').shape[0]* attributions.get('LRP').shape[1],20)\n",
        "DeepLIFT_ATTR = attributions.get('DeepLIFT').reshape(attributions.get('DeepLIFT').shape[0]* attributions.get('DeepLIFT').shape[1],20)\n",
        "IG_ATTR = attributions.get('IntegratedGradients').reshape(attributions.get('IntegratedGradients').shape[0]* attributions.get('IntegratedGradients').shape[1],20)\n",
        "\n",
        "attributions_long = {'Lime':LIME_ATTR,'KernelShap':KS_ATTR,'LRP':LRP_ATTR,'DeepLIFT':DeepLIFT_ATTR,'IntegratedGradients':'IG_ATTR'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "0wNQck4KC9Dr",
        "outputId": "51ab5d1a-6f19-439b-c7d6-edf457abfd5f"
      },
      "outputs": [],
      "source": [
        "# Use SHAP plot framework\n",
        "X_train_df = pd.DataFrame(X_train.cpu().detach().numpy(),columns = colnames)\n",
        "X_valid_df = pd.concat([pd.DataFrame(X_valid.cpu().detach().numpy(),columns = colnames)]*nr_iter)\n",
        "\n",
        "# compute fake SHAP values that will overwritten with the computed attributions\n",
        "Xtest,ytest = shap.datasets.adult()\n",
        "modeltest = xgboost.XGBClassifier().fit(Xtest, ytest)\n",
        "explainer = shap.Explainer(modeltest, X_train_df)\n",
        "shap_values = explainer(X_valid_df)\n",
        "\n",
        "attr = LRP_ATTR\n",
        "\n",
        "shap_values.values = attr\n",
        "shap_values.data = np.round((shap_values.data * stdX[np.newaxis,:]) + meanX[np.newaxis,:],4)\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.set_size_inches(10, 6)\n",
        "shap.plots.bar(shap_values, max_display=18, show_data=False, show = False)\n",
        "plt.xlabel('mean|LRP Attributions|')\n",
        "plt.xlim([0,0.6])\n",
        "#plt.savefig('/content/gdrive/MyDrive/MA_XAI/PatientData_ready/Results/Barplot_LRP.png',bbox_inches='tight')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "p8EtlCABfhxp",
        "outputId": "a3a9f999-19c1-4082-de10-306ea384db04"
      },
      "outputs": [],
      "source": [
        "# Box plots for different attributions\n",
        "def boxplot_avg_attributions (attr,title):\n",
        "  a0 = np.transpose(attr)\n",
        "  a0 = a0 / np.linalg.norm(a0, ord=1)\n",
        "  dic = dict(zip(colnames, a0))\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  fig.set_size_inches(8,6, forward = True)\n",
        "  boxplot = ax.boxplot(dic.values())\n",
        "  ax.set_xticklabels(dic.keys(), rotation = 90, ha = 'left')\n",
        "  ax.set_ylabel('Attribution')\n",
        "  #ax.set_title(title)\n",
        "  ax.set_ylim([-0.01,0.01])\n",
        "  ax.grid(False)\n",
        "  \n",
        "\n",
        "boxplot_avg_attributions(LRP_ATTR,'KernelSHAP')\n",
        "#plt.savefig('/content/gdrive/MyDrive/MA_XAI/PatientData_ready/Results/Boxplot_LRP_newScale.png',bbox_inches='tight')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "KlthNt2oXu5l",
        "outputId": "33476a00-979a-4cdf-c3c4-affc7f7664ae"
      },
      "outputs": [],
      "source": [
        "attr_tp = attributions_by_group.get('True Positive')\n",
        "attr_tn = attributions_by_group.get('True Negative')\n",
        "\n",
        "df = pd.DataFrame(np.concatenate([attr_tp,attr_tn]), columns=colnames )\n",
        "\n",
        "tp = ['True Positive']*len(attr_tp)\n",
        "tn = ['True Negative']*len(attr_tn)\n",
        "cat = tp + tn\n",
        "\n",
        "df['Model Outcomes'] = pd.Series(cat)\n",
        "\n",
        "# create boxplot\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize = (15,5), sharey = True)\n",
        "fig.suptitle('Attributions per Model Outcomes')\n",
        "\n",
        "sns.boxplot(ax = axes[0], x = 'Model Outcomes', y = 'gcs', data = df,palette=\"Greys\")\n",
        "axes[0].set_title('GCS')\n",
        "axes[0].set_ylabel('Attribution')\n",
        "sns.boxplot(ax = axes[1], x = 'Model Outcomes', y = 'sofatotal', data = df,palette=\"Greys\")\n",
        "axes[1].set_title('SOFA score')\n",
        "axes[1].set_ylabel('Attribution')\n",
        "sns.boxplot(ax = axes[2], x = 'Model Outcomes', y = 'pao2', data = df,palette=\"Greys\")\n",
        "axes[2].set_title('PaO2')\n",
        "axes[2].set_ylabel('Attribution')\n",
        "\n",
        "\n",
        "#plt.savefig('/content/gdrive/MyDrive/MA_XAI/PatientData_ready/Results/Boxplot_tptn.png',bbox_inches='tight')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjiu9ggdb2FV"
      },
      "outputs": [],
      "source": [
        "\"\"\" Helper function \"\"\"\n",
        "def attributions_sum(attr_test):\n",
        "    attr_test = attr_test.reshape(attr_test.shape[0]*attr_test.shape[1],20)\n",
        "    attr_test_sum = abs(attr_test).sum(0)\n",
        "    #attr_test_norm_sum = attr_test_sum / np.linalg.norm(attr_test_sum, ord=1)\n",
        "    \n",
        "    return attr_test_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptIQL_3VKky7"
      },
      "outputs": [],
      "source": [
        "attr_iter = pd.DataFrame()\n",
        "attr_iter['Features'] = colnames\n",
        "\n",
        "for name, attr_test in attributions.items():\n",
        "    attr_iter[name] = attributions_sum(attr_test)\n",
        "\n",
        "max = attr_iter[['Lime','KernelShap','LRP','DeepLIFT','IntegratedGradients']].max(axis=0).values\n",
        "min = attr_iter[['Lime','KernelShap','LRP','DeepLIFT','IntegratedGradients']].min(axis=0).values\n",
        "attr_iter[['Lime','KernelShap','LRP','DeepLIFT','IntegratedGradients']] = (attr_iter[['Lime','KernelShap','LRP','DeepLIFT','IntegratedGradients']] -min) / (max - min)\n",
        "\n",
        "\n",
        "attr_iter = pd.melt(attr_iter, id_vars='Features')\n",
        "    \n",
        "# save new attributions in final data frame\n",
        "#attr_iter.rename(columns = {'value': f'value{i}'},inplace = True)\n",
        "final_attr_df = pd.concat((attr_iter,attr_iter.loc[:,'value']), axis = 1)\n",
        "\n",
        "# Normalize logistic regression coefficients also between 0 and 1\n",
        "max_l = abs_logit_coef.max(axis=0).value\n",
        "min_l = abs_logit_coef.min(axis=0).value\n",
        "\n",
        "abs_logit_coef['value'] = (abs_logit_coef['value'] - min_l) / (max_l - min_l)\n",
        "\n",
        "# Attach coefficients from logistic regression\n",
        "attr_iter_f = pd.concat([attr_iter,abs_logit_coef])\n",
        "\n",
        "# Renaming for graph\n",
        "attr_iter_f.columns = ['Feature', 'Method', 'value']\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t_mpAMoIIWov",
        "outputId": "9547098a-7896-4e26-d580-271f49d8315b"
      },
      "outputs": [],
      "source": [
        "# visualize\n",
        "fig  = px.bar(\n",
        "    data_frame=attr_iter_f,\n",
        "    x='value',\n",
        "    y='Method',\n",
        "    facet_col='Feature',\n",
        "    facet_col_wrap=5,\n",
        "    color='Method',\n",
        "    orientation='h',\n",
        "    height=1000,\n",
        "    width=1200,\n",
        "    template='ggplot2'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
